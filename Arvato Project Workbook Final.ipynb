{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy import stats\n",
    "from time import time\n",
    "import os, math, time, random, datetime\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_df = azdias.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = customers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(azdias_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Analysis; \n",
    "### Data Exploration & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring AZDIAS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at descriptive stats on the Azdias dataset\n",
    "azdias_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking nulls in AZDIAS\n",
    "azdiasdf_null = azdias_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking % of missing attributes in each column\n",
    "azdiasdf_null_percent = azdiasdf_null / len(azdias_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise missing data\n",
    "(azdiasdf_null.sort_values(ascending=False)[:50].plot(title = 'AZDIAS columns with missing data ranked', kind='bar', figsize=(20,8), fontsize=13))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of empty data in fields by percentage\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(azdiasdf_null_percent, bins = np.linspace(10,100,19), facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('% of missing value')\n",
    "plt.ylabel('# of Columns')\n",
    "plt.title('Missing data distribution in each AZDIAS column')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# % of missing data in columns based on dataframe created earlier\n",
    "print('% of missing data in AZDIAS columns','\\n',azdiasdf_null_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring CUSTOMERS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at descriptive stats on the Customers dataset\n",
    "customers_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking nulls in Customers dataset\n",
    "customersdf_null = customers_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking % of missing attributes in each column\n",
    "customersdf_null_percent = customersdf_null / len(customers_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise missing data\n",
    "(customersdf_null.sort_values(ascending=False)[:50].plot(title = 'CUSTOMER columns with missing data ranked', kind='bar', figsize=(20,8), fontsize=12))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distribution of empty data in fields by percentage\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(customersdf_null_percent, bins = np.linspace(10,100,19), facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('% of missing value')\n",
    "plt.ylabel('# of Columns')\n",
    "plt.title('Missing data distribution in each CUSTOMERS column')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# % of missing data in columns based on dataframe created earlier\n",
    "print('% of missing data in columns','\\n',customersdf_null_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns from Customers that are not in Azdias\n",
    "\n",
    "customers_df.drop(columns=['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing columns with high null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns in AZDIAS that have over 65% of nulls in them \n",
    "az_column_nans = azdias_df.isnull().mean()\n",
    "drop_cols = azdias_df.columns[az_column_nans > 0.65]\n",
    "print('columns to drop: ', drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of missing data in columns\n",
    "print('% of missing data in columns','\\n',azdias_null_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of column in azdias before dropping: ', len(azdias_df.columns))\n",
    "azdias = azdias_df.drop(drop_cols,axis=1)\n",
    "print('# of column in azdias after dropping: ', len(azdias.columns))\n",
    "\n",
    "print('# of column in customers before dropping: ', len(customers_df.columns))\n",
    "customers = customers_df.drop(drop_cols,axis=1)\n",
    "print('# of column in customers after dropping: ', len(customers.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape of each df\n",
    "azdias.shape\n",
    "customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing columns that would make the model too sensitive to each based on a correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AZDIAS df - correlation matrix\n",
    "\n",
    "corr_matrix = azdias.corr().abs()\n",
    "sns.heatmap(corr_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking columns that have an upper correlation limit\n",
    "upper_limit = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns to drop based on those who are over the upper limit of .75\n",
    "drop_columns = [column for column in upper_limit.columns if any(upper_limit[column] > .75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns from azdias\n",
    "azdias = azdias.drop(drop_columns, axis=1)\n",
    "len(azdias.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CUSTOMERS df - correlation matrix\n",
    "\n",
    "cus_corr_matrix = customers.corr().abs()\n",
    "sns.heatmap(cus_corr_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking columns that have an upper correlation limit\n",
    "cus_upper_limit = corr_matrix.where(np.triu(np.ones(cus_corr_matrix.shape), k=1).astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns to drop based on threshold limit\n",
    "cus_drop_columns = [column for column in cus_upper_limit.columns if any(cus_upper_limit[column] > .75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns from azdias\n",
    "customers = customers.drop(cus_drop_columns, axis=1)\n",
    "len(customers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking columns for removal if they have too many unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking unique values of 'EINGEFUEGT_AM'\n",
    "len(azdias['EINGEFUEGT_AM'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute \"EINGEFUEGT_AM\" has too many unique values so it needs to be dropped\n",
    "\n",
    "azdias = azdias.drop(['EINGEFUEGT_AM'],axis=1)\n",
    "customers = customers.drop(['EINGEFUEGT_AM'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking unique values of 'D19_LETZTER_KAUF_BRANCHE\n",
    "len(azdias['D19_LETZTER_KAUF_BRANCHE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = azdias.drop(['D19_LETZTER_KAUF_BRANCHE'],axis=1)\n",
    "customers = customers.drop(['D19_LETZTER_KAUF_BRANCHE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(azdias.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(customers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying categorical fields \n",
    "\n",
    "cols = azdias.columns\n",
    "num_cols = azdias._get_numeric_data().columns\n",
    "print('num_cols: ',num_cols)\n",
    "print('categorical: ',list(set(cols) - set(num_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values are filled with -1 showing unknown\n",
    "\n",
    "azdias[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = azdias[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].replace(['X','XX'],-1)\n",
    "customers[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = customers[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].replace(['X','XX'],-1)\n",
    "azdias[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = azdias[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].fillna(-1)\n",
    "customers[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = customers[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].fillna(-1)\n",
    "azdias[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = azdias[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].astype(int)\n",
    "customers[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = customers[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].astype(int)\n",
    "azdias[['CAMEO_DEU_2015','OST_WEST_KZ']]=azdias[['CAMEO_DEU_2015','OST_WEST_KZ']].fillna(-1)\n",
    "customers[['CAMEO_DEU_2015','OST_WEST_KZ']]=customers[['CAMEO_DEU_2015','OST_WEST_KZ']].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with 9 for fields that has 9 marked as unknown\n",
    "\n",
    "azdias[azdias.columns[(azdias==9).any()]] = azdias[azdias.columns[(azdias==9).any()]].fillna(9)\n",
    "customers[customers.columns[(customers==9).any()]] = customers[customers.columns[(customers==9).any()]].fillna(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with 0 for fields that has 0 marked as unknown\n",
    "\n",
    "azdias[azdias.columns[(azdias==0).any()]] = azdias[azdias.columns[(azdias==0).any()]].fillna(0)\n",
    "customers[customers.columns[(customers==0).any()]] = customers[customers.columns[(customers==0).any()]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with -1 for fields that has -1 marked as unknown\n",
    "\n",
    "azdias[azdias.columns[(azdias==-1).any()]] = azdias[azdias.columns[(azdias==-1).any()]].fillna(-1)\n",
    "customers[customers.columns[(customers==-1).any()]] = customers[customers.columns[(customers==-1).any()]].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = pd.get_dummies(azdias)\n",
    "customers = pd.get_dummies(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_columns = azdias.columns\n",
    "customers_columns = customers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing nans with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now replacing missing values by using the most frequent value along each column. \n",
    "# We are using this because it can be used with strings or numeric data. \n",
    "#If there is more than one such value, only the smallest is returned.\n",
    "imputer = Imputer(missing_values='NaN',strategy='most_frequent',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = imputer.fit_transform(azdias)\n",
    "azdias = pd.DataFrame(azdias)\n",
    "azdias.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = imputer.fit_transform(customers)\n",
    "customers = pd.DataFrame(customers)\n",
    "customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to int\n",
    "azdias = azdias.astype(int)\n",
    "customers = customers.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all rows that have a value in a column that is 3 standard deviations away from the mean\n",
    "\n",
    "azdias = azdias[(np.abs(stats.zscore(azdias)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[(np.abs(stats.zscore(customers)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize & Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_A = StandardScaler(copy=False)\n",
    "az_scaled = sc_A.fit_transform(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df = pd.DataFrame(az_scaled,columns= azdias_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df = az_df.set_index('LNR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df.to_pickle('azdias_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_scaled = sc_A.fit_transform(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_df = pd.DataFrame(cus_scaled,columns= customers_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_df = cus_df.set_index('LNR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_df.to_pickle('customers_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the above working a Preprocess pipeline is created making it easier to run once the data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_2(df, for_clustering, df_name=None):    \n",
    "    \"\"\"\n",
    "    Runs the pre-processing steps that have been tested and worked, and uses the cleaned processed data for clustering. \n",
    "    \n",
    "    INPUT:\n",
    "    - df: Azdias and Customers dataframe\n",
    "    - for_clustering: processed data to be used for clustering \n",
    "    \n",
    "    OUTPUT:\n",
    "    Dataframes that have:\n",
    "    - Invaluable & highly correlated columns dropped, \n",
    "    - Imputed nulls and missing values\n",
    "    - Encoded & scaled data\n",
    "    \"\"\"\n",
    "    \n",
    "    if for_clustering:\n",
    "        if df_name == 'azdias':\n",
    "            # 73% of rows kept their data with various missing data points \n",
    "            df = df[df.isnull().sum(axis=1) <= 27].reset_index(drop=True)\n",
    "        elif df_name == 'customers':            \n",
    "            #dropped these columns as they weren't in AZDIAS\n",
    "            df.drop(columns=['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], inplace=True)\n",
    "        \n",
    "    #Dropping cols that have a high % of missing data \n",
    "    drop_cols = ['ALTER_KIND1', 'ALTER_KIND2', 'ALTER_KIND3', 'ALTER_KIND4', 'EXTSEL992','KK_KUNDENTYP']\n",
    "    \n",
    "    df = df.drop(drop_cols,axis=1)\n",
    "    \n",
    "    #Dropping cols that have too many unique values \n",
    "    df = df.drop(['EINGEFUEGT_AM'],axis=1)\n",
    "    df = df.drop(['D19_LETZTER_KAUF_BRANCHE'],axis=1)\n",
    "\n",
    "\n",
    "    # Correlation Matrix to identify cols that will make the model too sensitive   \n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper_limit = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Identifying columns to drop based on upper limit\n",
    "    drop_cols = [column for column in upper_limit.columns if any(upper_limit[column] > .75)]\n",
    "    # Dropping cols\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    print('shape after corr', df.shape)\n",
    "\n",
    "\n",
    "    # Missing Values filled with -1 indicating unknown \n",
    "    df[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = df[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].replace(['X','XX'],-1)\n",
    "    df[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = df[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].fillna(-1)\n",
    "    df[['CAMEO_DEUG_2015','CAMEO_INTL_2015']] = df[['CAMEO_DEUG_2015','CAMEO_INTL_2015']].astype(int)\n",
    "    df[['CAMEO_DEU_2015','OST_WEST_KZ']]=df[['CAMEO_DEU_2015','OST_WEST_KZ']].fillna(-1)\n",
    "\n",
    "    # fillna with 9 for fields that has 9 marked as unknown\n",
    "    df[df.columns[(df==9).any()]] = df[df.columns[(df==9).any()]].fillna(9)\n",
    "\n",
    "    # fillna with 0 for fields that has 0 marked as unknown\n",
    "    df[df.columns[(df==0).any()]] = df[df.columns[(df==0).any()]].fillna(0)\n",
    "\n",
    "    # fillna with -1 for fields that has 0 marked as unknown\n",
    "    df[df.columns[(df==-1).any()]] = df[df.columns[(df==-1).any()]].fillna(-1)\n",
    "\n",
    "     \n",
    "\n",
    "    #Encoding data via one hot encoding - required for model training and testing\n",
    "    df = pd.get_dummies(df)\n",
    "    print('shape after one-hot', df.shape)\n",
    "    \n",
    "    df_cols = list(df.columns.values)\n",
    "\n",
    "    # Imputing Nans with mode value - using 'most frequent'c an be used with strings or numeric data, \n",
    "    #only smallest value will be returned\n",
    "    imputer = Imputer(missing_values='NaN',strategy='most_frequent',axis=0)\n",
    "    df = imputer.fit_transform(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    print('shape after impute', df.shape)\n",
    "    \n",
    "    # Convert to int\n",
    "    df = df.astype(int)\n",
    "\n",
    "    # Removing all rows that have a value in a column that is 3 standard deviations away from the mean\n",
    "    if for_clustering:\n",
    "        print('inside outliers if')\n",
    "        df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)] \n",
    "        print('shape before scaling', df.shape)\n",
    "        \n",
    "    # Standardizing and scaling the data \n",
    "    scale = StandardScaler(copy=False)\n",
    "    scaled = scale.fit_transform(df)\n",
    "    df = pd.DataFrame(scaled,columns= df_cols)\n",
    "    print('shape after scaling', df.shape)\n",
    "        \n",
    "    df = df.set_index('LNR')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying preprocess on AZDIAS\n",
    "azdias = data_preprocess_2(azdias, True, 'azdias')\n",
    "print(azdias.shape)\n",
    "print(azdias.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying preprocess on CUSTOMERS\n",
    "customers = data_preprocess_2(customers, True, 'customers')\n",
    "print(customers.shape)\n",
    "print(customers.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA): As there is a high volume of dimensions, we need to reduce these and the method we will use is PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Scree Plot to show the variance explained by each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(azdias)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Explained Variance Ratio - AZDIAS')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the Scree plot, it shows that a good number of components to use is: 200\n",
    "def reduce_dims(df,n=200):\n",
    "    pca = PCA(n_components=n).fit(df)\n",
    "    reduced_dims = pca.transform(df)\n",
    "    reduced_dims = pd.DataFrame(reduced_dims)\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    return reduced_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the variance in the data explained by the principal components after running PCA\n",
    "reduced_azdias = reduce_dims(azdias)\n",
    "reduced_azdias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_customers = reduce_dims(customers)\n",
    "reduced_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering will be used to to describe the required relationship, with the Elbow Method being used to determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(1, 25))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, init='k-means++')\n",
    "    km.fit(reduced_azdias.sample(20000))\n",
    "    sse.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sse against k\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(list_k, sse, linestyle='-', marker='x', color='navy')\n",
    "plt.title('k-means Clustering Elbow Plot')\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Avg within-cluster dstance\")\n",
    "plt.xticks(list(range(1,25)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scree plot shows that near theCluster '12' mark, the average distance tends to flatten out - so for this reason, 12 clusters will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 12\n",
    "kmeans = KMeans(n_clusters=n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_preds = kmeans.fit_predict(reduced_azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_clustered = pd.DataFrame(azdias_preds, columns = ['Cluster'])\n",
    "\n",
    "azdias_clustered.to_pickle('azdias_clustered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_preds = kmeans.fit_predict(reduced_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_clustered = pd.DataFrame(customers_preds, columns = ['Cluster'])\n",
    "\n",
    "customers_clustered.to_pickle('customers_clustered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number in each population segment - AZDIAS\n",
    "population_clusters = pd.Series(azdias_preds)\n",
    "pc = population_clusters.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of predictions for each customer segment - CUSTOMERS\n",
    "customer_clusters = pd.Series(customers_preds)\n",
    "cc = customer_clusters.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from population and customer segments\n",
    "\n",
    "summary_df = pd.concat([pc, cc], axis=1).reset_index()\n",
    "\n",
    "summary_df.columns = ['Cluster Number','General Population','Customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding percentage columns\n",
    "\n",
    "summary_df['% of Total Pop'] = ( summary_df['General Population'] / (summary_df['General Population'].sum()) * 100 ).round(2)\n",
    "\n",
    "summary_df['% of Total Customer'] = ( summary_df['Customer'] / (summary_df['Customer'].sum()) * 100 ).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax = summary_df['% of Total Pop'].plot(x=summary_df['Cluster Number'], width=-0.3, align='edge', color='navy', kind='bar', position=0)\n",
    "ax = summary_df['% of Total Customer'].plot(kind='bar', color='green', width = 0.3, align='edge', position=1)\n",
    "\n",
    "ax.set_xlabel('Clusters', fontsize=15) \n",
    "ax.set_ylabel('% Ratio between Population & Customers', fontsize=15)\n",
    "\n",
    "ax.xaxis.set(ticklabels=range(20))\n",
    "ax.tick_params(axis = 'x', which = 'major', labelsize = 13)\n",
    "ax.margins(x=0.5,y=0.1)\n",
    "\n",
    "plt.legend(('General Population (AZDIAS)', 'Customer (CUSTOMERS)'), fontsize=15)\n",
    "plt.title(('% of General Population & Customer in Each Cluster'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above bar chart shows how the clusters are distributed across the 2 datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population clusters show only small differences in size, however the Customers clusters show a large imbalance, specifcally in Cluster 6 (over-representation) & Cluster 9 (under-representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mailout_train.drop('RESPONSE',axis=1)\n",
    "y = mailout_train['RESPONSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "df_mailout_train  = data_preprocess_2(X, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailout_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_mailout_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'RandomForestClassifier': RandomForestClassifier(), \n",
    "          'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(df):\n",
    "    \"\"\"\n",
    "    Returns randomized X and y.\n",
    "    \n",
    "    Input: DataFrame\n",
    "    Output: randomized X and y\n",
    "    \"\"\"\n",
    "    \n",
    "    df_randomized = df.sample(frac=1) #frac = 1 will take a random sample of the whole df\n",
    "    y_rand = df_randomized['RESPONSE']\n",
    "    X_rand = df_randomized.drop(['RESPONSE'],axis=1)\n",
    "    return X_rand, y_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_learning_curves(X, y, estimator, num_trainings):\n",
    "    \"\"\"\n",
    "    Visualise a learning curve that shows the validation and training auc_score of an estimator \n",
    "    depending on the number of training samples.\n",
    "    \n",
    "    Input:\n",
    "        X: array \n",
    "        y: array \n",
    "        estimator: o that implements the “fit” and “predict” methods\n",
    "        num_trainings (int): number of training samples to plot\n",
    "        \n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, scoring = 'roc_auc', train_sizes=np.linspace(.1, 1.0, num_trainings))\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    print(\"AUC train score = {}\".format(train_scores_mean[-1].round(2)))\n",
    "    print(\"AUC validation score = {}\".format(test_scores_mean[-1].round(2)))\n",
    "    plt.grid()\n",
    "\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"% of training set\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    plt.plot(np.linspace(.1, 1.0, num_trainings)*100, train_scores_mean, 'o-', color=\"n\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(np.linspace(.1, 1.0, num_trainings)*100, test_scores_mean, 'o-', color=\"o\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.yticks(np.arange(0.45, 1.02, 0.05))\n",
    "    plt.xticks(np.arange(0., 100.05, 10))\n",
    "    plt.legend(loc=\"best\")\n",
    "    print(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_key in models.keys():\n",
    "    print(model_key)\n",
    "    ml_pipeline = Pipeline([\n",
    "        ('transform', column_transformer),\n",
    "        ('model', models[model_key])\n",
    "    ])\n",
    "    X, y = randomize(mailout_train_clean)\n",
    "    vis_learning_curves(X, y, ml_pipeline, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Random Forest Classifier shows a low valisation score and high training score, implying the model is overfitted on traning and has bias.  \n",
    "#### Gradient Boosting Classifier shows decreasing model bias when the sample size is increased. Based on this and the high validation score - the GBC will be the estimator for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize with GradientBoostingClassifier\n",
    "gbc_pipeline = Pipeline([\n",
    "    ('transform', column_transformer),\n",
    "    ('gbc', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "parameters = {'gbc__learning_rate': [0.1], 'gbc__n_estimators': [100],\n",
    "             'gbc__max_depth': [5], 'gbc__min_samples_split': [2]}        \n",
    "        \n",
    "grid_gbc = GridSearchCV(gbc_pipeline, parameters, scoring = 'roc_auc', verbose=50)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_gbc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the estimator and predict\n",
    "print(grid_gbc.best_params_)\n",
    "best_clf = grid_gbc.best_estimator_\n",
    "best_predictions = best_clf.predict_proba(X)[:, 1]\n",
    "\n",
    "print(\"ROC score:\".format(roc_auc_score(y, best_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_clf.named_steps['gbc'].feature_importances_\n",
    "pd.Series(feature_importances, index=[''] + column_names).sort_values()[-20:].plot(kind='barh', figsize=(20,10))\n",
    "plt.xlabel('feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
